# -*- coding: utf-8 -*-
"""churn_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-t3HhLBzQkAC64NRTJqbwzLm625cm8hx

## TELCO COMPANY CUSTOMER CHURN PREDICTION

# importing Neccesary Libraries
"""

!pip install plotly

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.graph_objects as go
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split,cross_val_score,StratifiedKFold
from sklearn.metrics import classification_report
from sklearn.ensemble import RandomForestClassifier
import plotly.express as px
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings("ignore")

"""# Loading the dataset"""

data=pd.read_csv('Customer-Churn.csv')



"""# Data Analysis"""

data.head(5)

data.info()

data['TotalCharges'] = pd.to_numeric(data.TotalCharges, errors='coerce')

data.info()

data['PaymentMethod'].unique()

data['Contract'].unique()

data.select_dtypes(include=['number']).corr()

data=data.drop('customerID', axis=1)



data.isnull().sum()

data=data.dropna()

data.isnull().sum()

data['Churn'].value_counts()

"""# performing visualization of the dataset"""

fig = px.histogram(data, x="Churn",   title="<b>Chrun distribution</b>")
fig.update_layout(width=700, height=500, bargap=0.1)
fig.show()

fig = px.histogram(data, x="SeniorCitizen", title="<b>SeniorCitizen distribution</b>")
fig.update_layout(width=800, height=400, bargap=0.1)
fig.show()

fig = px.histogram(data, x="Churn", color="TechSupport",barmode="group",  title="<b>Chrun distribution w.r.t. TechSupport</b>")
fig.update_layout(width=800, height=400, bargap=0.1)
fig.show()

data.columns

fig = px.histogram(data, x="Churn", color="PhoneService",barmode="group",  title="<b>Chrun distribution w.r.t. PhoneService</b>")
fig.update_layout(width=700, height=500, bargap=0.1)
fig.show()

labels = data['PaymentMethod'].unique()
values = data['PaymentMethod'].value_counts()
fig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.3)])
fig.update_layout(title_text="<b>Payment Method Distribution</b>")
fig.show()

fig = px.histogram(data, x="tenure", color="Churn",barmode="group",  title="<b>Chrun distribution w.r.t. tenure</b>")
fig.update_layout(width=800, height=500, bargap=0.1)
fig.show()

"""# Data Preprocessing"""

object_columns = data.select_dtypes(include='object').columns
object_columns= list(object_columns)
print(object_columns)



from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
for x in object_columns:
    data[x]=le.fit_transform(data[x])
data.head()

plt.figure(figsize=(15,12))
sns.heatmap(data.corr(),cmap='terrain')

data.info()

x=data.iloc[:, :-1]
y=data.iloc[:,-1]

x

y

"""# Feature Selection"""

rf = RandomForestClassifier()
rf.fit(x,y)
feature_importances = rf.feature_importances_
importance_df = pd.DataFrame({'Feature': x.columns, 'Importance': feature_importances})
importance_df = importance_df.sort_values(by='Importance', ascending=False)
ifs=importance_df['Feature'][:5]
selected_columns = list(ifs)
print(selected_columns)

plt.figure(figsize=(13,7))
plt.bar(ifs, importance_df['Importance'][:5], color='black')
plt.xlabel('Features')
plt.ylabel('Importance')
plt.title('Feature Importances')
plt.xticks(rotation=90)
plt.show()

data2=data[['TotalCharges', 'MonthlyCharges', 'tenure', 'Contract', 'PaymentMethod','Churn']]

x=data[['TotalCharges', 'MonthlyCharges', 'tenure', 'Contract', 'PaymentMethod']]

x

from imblearn.over_sampling import SMOTE
smote = SMOTE()
x, y = smote.fit_resample(x, y)



x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.20)

sc=StandardScaler()
X_train = sc.fit_transform(x_train)
X_test = sc.transform(x_test)

lr=LogisticRegression()
kFold=StratifiedKFold(n_splits=5)
cv_scores = cross_val_score(lr, x_train, y_train, cv=kFold, scoring='accuracy')
print(f"Cross-validation scores: {cv_scores}")
print(f"Mean accuracy: {cv_scores.mean()}")
lr.fit(x_train,y_train)
predictions = lr.predict(x_test)
print(classification_report(predictions,y_test))

dt=DecisionTreeClassifier()
kFold=StratifiedKFold(n_splits=5)
cv_scores = cross_val_score(dt, x_train, y_train, cv=kFold, scoring='accuracy')
print(f"Cross-validation scores: {cv_scores}")
print(f"Mean accuracy: {cv_scores.mean()}")
dt.fit(x_train,y_train)
predictions = dt.predict(x_test)
print(classification_report(predictions,y_test))

svm=SVC()
kFold=StratifiedKFold(n_splits=5)
cv_scores = cross_val_score(svm, x_train, y_train, cv=kFold, scoring='accuracy')
print(f"Cross-validation scores: {cv_scores}")
print(f"Mean accuracy: {cv_scores.mean()}")
svm.fit(x_train,y_train)
predictions = svm.predict(x_test)
print(classification_report(predictions,y_test))

rf=RandomForestClassifier()
kFold=StratifiedKFold(n_splits=5)
cv_scores = cross_val_score(rf, x_train, y_train, cv=kFold, scoring='accuracy')
print(f"Cross-validation scores: {cv_scores}")
print(f"Mean accuracy: {cv_scores.mean()}")
rf.fit(x_train,y_train)
predictions = rf.predict(x_test)
print(classification_report(predictions,y_test))

data2

rf.predict([[108.15, 53.85, 2, 0, 3]])

rf.predict([[1889.50, 56.95, 34, 1, 3]])

rf.predict([[306.60, 74.40, 4, 0, 3]])

rf.predict([[6844.50, 105.65, 66, 2, 0]])

import pickle
import gzip
import numpy as np

with gzip.open('model.pkl.gz', 'wb') as f:
    pickle.dump(rf, f)

# with gzip.open('model.pkl.gz', 'rb') as f:
#     model = pickle.load(f)



